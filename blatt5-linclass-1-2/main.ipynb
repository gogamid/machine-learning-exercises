{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os  ## download MNIST if not present in current dir!\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "if os.path.exists(\"./mnist.npz\") == False:\n",
    "    print(\"Downloading MNIST...\");\n",
    "    fname = 'mnist.npz'\n",
    "    url = 'http://www.gepperth.net/alexander/downloads/'\n",
    "    r = requests.get(url + fname)\n",
    "    open(fname, 'wb').write(r.content)\n",
    "\n",
    "## read it into\n",
    "data = np.load(\"mnist.npz\")\n",
    "traind = data[\"arr_0\"];\n",
    "trainl = data[\"arr_2\"];\n",
    "traind = traind.reshape(60000, 784)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:46:18.069747Z",
     "start_time": "2023-11-12T18:46:17.599168Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class LinClass(object):\n",
    "\n",
    "    ## allocate weight and bias arrays and store ref 2 train data/labels\n",
    "    def __init__(self, n_in, n_out, X, T):\n",
    "        self.W = np.ones([n_in, n_out]) * 0.1\n",
    "        self.b = np.ones([1, n_out]) * 0.1\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.N = X.shape[0]\n",
    "\n",
    "    # normal cross-entropy loss. Fill in your code here! [b5p4-cross-entropy\n",
    "    def loss(self, Y, T):\n",
    "        return -(T * np.log(Y))\n",
    "\n",
    "    def dLdb(self, Y, T):\n",
    "        print()\n",
    "\n",
    "    # fill in your code here!\n",
    "    def dLdW(self, X, Y, T):\n",
    "        print()\n",
    "\n",
    "    # softmax: fill in your code here! [b5p2-softmax]\n",
    "    def sofmax(self, X):\n",
    "        ex = np.exp(X)\n",
    "        return ex / ex.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # dummy model, fill in your code! [b5p3-lin-classifier]\n",
    "    def lin_class(self, X):\n",
    "        return self.sofmax(np.matmul(X, self.W) + self.b)\n",
    "\n",
    "    # performs a single gradient descent step\n",
    "    # works with any size of X and T\n",
    "    def train_step(self, X, T, eps):\n",
    "        print()\n",
    "\n",
    "    # perform multiple gradient descent steps and display loss. Does it go down??\n",
    "    def train(self, max_it, eps):\n",
    "        print()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T21:43:01.897822Z",
     "start_time": "2023-11-08T21:43:01.888297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S is:\n",
      " [[0.00246652 0.00246652 0.99506695]\n",
      " [0.21194156 0.21194156 0.57611688]]\n",
      "Checking S... [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# softmax\n",
    "x = np.array([[-1, -1, 5], [1, 1, 2.]])\n",
    "lc = LinClass(784, 10, traind, trainl)\n",
    "y = lc.sofmax(x)\n",
    "print(\"S is:\\n\", lc.sofmax(x))\n",
    "print(\"Checking S...\", y.sum(axis=1))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T21:43:04.123094Z",
     "start_time": "2023-11-08T21:43:04.091656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f is  [[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# lin class\n",
    "print(\"f is \", lc.lin_class(traind[1, :]));"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T21:43:06.318972Z",
     "start_time": "2023-11-08T21:43:06.315496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       -inf        -inf        -inf -0.69314718        -inf -1.2039728\n",
      "        -inf -1.60943791        -inf        -inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/g_tfwpps4wjfn0z8_1jjkch40000gq/T/ipykernel_27703/2643684038.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(Y)\n"
     ]
    }
   ],
   "source": [
    "X = [0, 0, 0, 0.5, 0, 0.3, 0, 0.2, 0, 0]\n",
    "T = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "r = lc.loss(X, T)\n",
    "print(r)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T21:56:03.755170Z",
     "start_time": "2023-11-08T21:56:03.746491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
